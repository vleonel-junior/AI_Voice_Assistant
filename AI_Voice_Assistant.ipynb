{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import whisper\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RBwfuU-jnt4M"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI, ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ChatMessageHistory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IrCKB8ZP2JaW"
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDq8c66o0deacxO67PMouAiZlBFOT_Ao4c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l8xSOVH62LJj"
   },
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()  # Charge la clé depuis un fichier .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aa18O8aKSZGV"
   },
   "outputs": [],
   "source": [
    "def chunk_embedder(model: str = \"models/embedding-001\", temperature: float = 0.5) -> GoogleGenerativeAIEmbeddings:\n",
    "    \"\"\"Initialise le modèle d'embeddings Google.\"\"\"\n",
    "    return GoogleGenerativeAIEmbeddings(model=model, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-YiiJFBwSZJA"
   },
   "outputs": [],
   "source": [
    "def load_llm(temperature: float = 0) -> ChatGoogleGenerativeAI:\n",
    "    \"\"\"Charge le modèle de langage ChatGoogleGenerativeAI.\"\"\"\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    return ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xclR8_YQU3Q_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7yvLKKv3U3UA"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def clear_persist_directory(persist_directory: str):\n",
    "    \"\"\"\n",
    "    Supprime tout le contenu du répertoire de persistance pour ChromaDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(persist_directory):\n",
    "            shutil.rmtree(persist_directory)\n",
    "            print(f\"Le répertoire '{persist_directory}' a été supprimé avec succès.\")\n",
    "        else:\n",
    "            print(f\"Le répertoire '{persist_directory}' est déjà vide ou n'existe pas.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la suppression du répertoire '{persist_directory}' : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KoLYe7yrU3W6"
   },
   "outputs": [],
   "source": [
    "def init_chroma_vectorstore(embeddings: GoogleGenerativeAIEmbeddings,\n",
    "                            csv_path: str,\n",
    "                            persist_directory: str,\n",
    "                            reset_db: bool = False) -> Chroma:\n",
    "    \"\"\"\n",
    "    Initialise ou charge un vectorstore ChromaDB, avec option de réinitialisation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Réinitialiser la base si demandé\n",
    "        if reset_db:\n",
    "            clear_persist_directory(persist_directory)\n",
    "\n",
    "        # S'assurer que le répertoire de persistance existe\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "        # Charger les documents\n",
    "        print(f\"Chargement du fichier CSV : {csv_path}\")\n",
    "        loader = CSVLoader(csv_path, encoding=\"utf-8\")\n",
    "        content = loader.load()\n",
    "\n",
    "        # Diviser les documents\n",
    "        print(\"Division des documents...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=20)\n",
    "        docs = text_splitter.split_documents(documents=content)\n",
    "\n",
    "        # Vérifier si une base de données existe déjà\n",
    "        chroma_db_path = os.path.join(persist_directory, \"chroma.sqlite3\")\n",
    "        if os.path.exists(chroma_db_path):\n",
    "            print(\"Chargement de la base existante...\")\n",
    "            vector_db = Chroma(\n",
    "                persist_directory=persist_directory,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "        else:\n",
    "            print(\"Création d'une nouvelle base vectorielle...\")\n",
    "            vector_db = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "\n",
    "        print(\"Base vectorielle initialisée avec succès.\")\n",
    "        return vector_db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'initialisation de la base vectorielle : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AXmAL2ygU3Z8"
   },
   "outputs": [],
   "source": [
    "def init_chat() -> ConversationalRetrievalChain:\n",
    "    \"\"\"\n",
    "    Initialise une chaîne de conversation avec ChromaDB comme backend.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Chargement du modèle LLM et des embeddings...\")\n",
    "        llm = load_llm()\n",
    "        embedding = chunk_embedder()\n",
    "\n",
    "        # Chemins pour les données\n",
    "        csv_path = \"/home/leonel/Assistant-AI/Service_Client_Cotonou.csv\"\n",
    "        vector_db_path = \"/home/leonel/Assistant-AI/vector_db\"\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"Le fichier CSV '{csv_path}' est introuvable.\")\n",
    "\n",
    "        # Initialisation de la base vectorielle\n",
    "        print(\"Initialisation de la base vectorielle...\")\n",
    "        vectorstore = init_chroma_vectorstore(\n",
    "            embeddings=embedding,\n",
    "            csv_path=csv_path,\n",
    "            persist_directory=vector_db_path,\n",
    "            reset_db=True  # Modifiez selon vos besoins\n",
    "        )\n",
    "\n",
    "        # Template de prompt\n",
    "        template = \"\"\"\n",
    "        Tu es un assistant de service client intelligent.\n",
    "\n",
    "        Historique de la conversation (si pertinent) :\n",
    "        {chat_history}\n",
    "\n",
    "        Contexte pertinent des documents (si disponible) :\n",
    "        {context}\n",
    "\n",
    "        Question actuelle : {question}\n",
    "\n",
    "        Réponse :\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\", \"chat_history\"])\n",
    "\n",
    "        # Mémoire de conversation\n",
    "        chat_memory = ConversationBufferWindowMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            chat_memory=ChatMessageHistory(),\n",
    "            output_key=\"answer\",\n",
    "            k=5,\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "        # Chaîne de conversation\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        conversation = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=chat_memory,\n",
    "            combine_docs_chain_kwargs={\n",
    "                \"prompt\": prompt,\n",
    "                \"document_variable_name\": \"context\",\n",
    "            },\n",
    "            return_source_documents=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "\n",
    "        print(\"Chaîne de conversation initialisée avec succès.\")\n",
    "        return conversation\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'initialisation de la chaîne de conversation : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rL1DKF5LU3c5"
   },
   "outputs": [],
   "source": [
    "def Talker(query: str, conversation: ConversationalRetrievalChain) -> str:\n",
    "    \"\"\"\n",
    "    Gère une interaction utilisateur avec le modèle, intégrant la gestion de l'historique\n",
    "    pour les questions ambiguës et le traitement des questions multiples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Priorité : Vérification des questions liées à l'historique\n",
    "        if \"que t'ai\" in query.lower() or \"dernière question\" in query.lower():\n",
    "            print(\"Priorité donnée à l'historique pour cette question.\")\n",
    "            messages = conversation.memory.chat_memory.messages\n",
    "            if len(messages) >= 2:\n",
    "                # Retourner le dernier message utilisateur avant la question actuelle\n",
    "                previous_query = messages[-2].content\n",
    "                return f\"Votre dernière question était : '{previous_query}'\"\n",
    "            else:\n",
    "                return \"Il n'y a pas encore assez d'historique pour répondre à cette question.\"\n",
    "\n",
    "        # Gestion des questions multiples dans une seule requête\n",
    "        questions = [q.strip() for q in query.split(\" et \")]\n",
    "        if len(questions) > 1:\n",
    "            print(\"Détection de plusieurs questions dans une seule requête. Traitement individuel.\")\n",
    "            responses = []\n",
    "            for q in questions:\n",
    "                # Traitement de chaque sous-question indépendamment\n",
    "                output = conversation({\"question\": q})\n",
    "                response = output.get('answer')\n",
    "                responses.append(f\"Pour '{q}': {response}\")\n",
    "\n",
    "                # Afficher les documents récupérés pour chaque question (pour diagnostic)\n",
    "                \"\"\"\n",
    "                source_documents = output.get('source_documents', [])\n",
    "                print(\"\\nDocuments récupérés pour le contexte (pour la sous-question) :\")\n",
    "                for i, doc in enumerate(source_documents):\n",
    "                    print(f\"Document {i+1} :\\n{doc.page_content}\")\n",
    "                \"\"\"\n",
    "\n",
    "            # Combiner les réponses pour chaque sous-question\n",
    "            return \"\\n\".join(responses)\n",
    "\n",
    "        # Si une seule question, traitement standard\n",
    "        output = conversation({\"question\": query})\n",
    "        response = output.get('answer')\n",
    "\n",
    "        # Afficher les documents récupérés pour diagnostic\n",
    "        \"\"\"\n",
    "        source_documents = output.get('source_documents', [])\n",
    "        print(\"\\nDocuments récupérés pour le contexte :\")\n",
    "        for i, doc in enumerate(source_documents):\n",
    "            print(f\"Document {i+1} :\\n{doc.page_content}\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Afficher l'historique actuel pour diagnostic\n",
    "        print(\"\\nHistorique actuel :\", conversation.memory.chat_memory.messages)\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de la requête : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"Désolé, une erreur s'est produite lors du traitement de votre demande.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w5Pi5jZmibEM"
   },
   "outputs": [],
   "source": [
    "def upload_audio_file():\n",
    "    \"\"\"\n",
    "    Importer un fichier audio depuis l'ordinateur de l'utilisateur\n",
    "    en utilisant une boîte de dialogue de sélection de fichier\n",
    "    \"\"\"\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    \n",
    "    # Créer une fenêtre racine Tkinter (elle sera cachée)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Cache la fenêtre principale\n",
    "    \n",
    "    print(\"Sélectionnez votre fichier audio (.wav ou .mp3)\")\n",
    "    \n",
    "    # Ouvrir la boîte de dialogue de sélection de fichier\n",
    "    filetypes = [\n",
    "        ('Fichiers audio', '*.wav;*.mp3'),\n",
    "        ('Fichiers WAV', '*.wav'),\n",
    "        ('Fichiers MP3', '*.mp3'),\n",
    "        ('Tous les fichiers', '*.*')\n",
    "    ]\n",
    "    \n",
    "    audio_file = filedialog.askopenfilename(\n",
    "        title=\"Sélectionnez un fichier audio\",\n",
    "        filetypes=filetypes\n",
    "    )\n",
    "    \n",
    "    if not audio_file:\n",
    "        print(\"Aucun fichier sélectionné.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Fichier sélectionné : {audio_file}\")\n",
    "    return audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZIr1kw4YihTt"
   },
   "outputs": [],
   "source": [
    "def speech_to_text(audio_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Convertit un fichier audio en texte en utilisant le modèle Whisper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import whisper\n",
    "        model = whisper.load_model(\"base\")\n",
    "\n",
    "        print(\"Transcription en cours...\")\n",
    "        result = model.transcribe(audio_file)\n",
    "\n",
    "        return result[\"text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la transcription : {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fsJRgBiihbl"
   },
   "outputs": [],
   "source": [
    "def play_text_to_speech_colab(text, language='fr', slow=False):\n",
    "    \"\"\"\n",
    "    Convertit du texte en audio et le lit dans Colab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from gtts import gTTS\n",
    "        from IPython.display import Audio\n",
    "        import os\n",
    "\n",
    "        # Convertir le texte en audio\n",
    "        tts = gTTS(text=text, lang=language, slow=slow)\n",
    "        temp_audio_file = \"/home/leonel/Assistant-AI/temp_audio.mp3\"\n",
    "        tts.save(temp_audio_file)\n",
    "\n",
    "        # Lecture de l'audio\n",
    "        display(Audio(temp_audio_file, autoplay=True))\n",
    "\n",
    "        # Nettoyage\n",
    "        os.remove(temp_audio_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture audio : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lolrSw4MgxLH"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale pour tester la conversation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Initialisation de la conversation...\")\n",
    "        conversation = init_chat()\n",
    "\n",
    "        print(\"\\nAssistant prêt. Téléchargez un fichier audio contenant votre question ou tapez 'quit' pour sortir.\\n\")\n",
    "        while True:\n",
    "            print(\"\\nOptions :\")\n",
    "            print(\"1. Télécharger un fichier audio pour poser une question.\")\n",
    "            print(\"2. Quitter.\")\n",
    "            choix = input(\"Votre choix : \").strip()\n",
    "\n",
    "            if choix == '2' or choix.lower() == 'quit':\n",
    "                print(\"Au revoir !\")\n",
    "                break\n",
    "\n",
    "            elif choix == '1':\n",
    "                # Étape 1 : Télécharger un fichier audio\n",
    "                audio_filename = upload_audio_file()\n",
    "                if not audio_filename:\n",
    "                    print(\"Aucun fichier n'a été téléchargé. Réessayez.\")\n",
    "                    continue\n",
    "\n",
    "                # Étape 2 : Transcrire l'audio en texte\n",
    "                query = speech_to_text(audio_filename)\n",
    "                if not query.strip():\n",
    "                    print(\"Impossible de transcrire l'audio. Réessayez.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Question transcrite : {query}\")\n",
    "\n",
    "                # Étape 3 : Obtenir la réponse\n",
    "                response = Talker(query, conversation)\n",
    "                print(\"Réponse :\", response)\n",
    "\n",
    "                # Étape 4 : Lecture de la réponse en audio\n",
    "                play_text_to_speech_colab(response, language='fr')\n",
    "\n",
    "            else:\n",
    "                print(\"Choix invalide. Veuillez sélectionner une option valide.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans la fonction principale : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eFKlHuhmU3jD",
    "outputId": "58dda17a-8710-4b91-c0d3-b00821c50284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation de la conversation...\n",
      "Chargement du modèle LLM et des embeddings...\n",
      "Initialisation de la base vectorielle...\n",
      "Le répertoire '/home/leonel/Assistant-AI/vector_db' est déjà vide ou n'existe pas.\n",
      "Chargement du fichier CSV : /home/leonel/Assistant-AI/Service_Client_Cotonou.csv\n",
      "Division des documents...\n",
      "Création d'une nouvelle base vectorielle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13067/1963190582.py:43: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationBufferWindowMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base vectorielle initialisée avec succès.\n",
      "Chaîne de conversation initialisée avec succès.\n",
      "\n",
      "Assistant prêt. Téléchargez un fichier audio contenant votre question ou tapez 'quit' pour sortir.\n",
      "\n",
      "\n",
      "Options :\n",
      "1. Télécharger un fichier audio pour poser une question.\n",
      "2. Quitter.\n",
      "Sélectionnez votre fichier audio (.wav ou .mp3)\n",
      "Fichier sélectionné : /home/leonel/Assistant-AI/ContacterServiceClient .mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [24:39<00:00, 98.2kiB/s]\n",
      "/home/leonel/anaconda3/envs/audio_env/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonel/anaconda3/envs/audio_env/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question transcrite :  Comment puis-je contacter le service client?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13067/2795526111.py:41: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = conversation({\"question\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Historique actuel : [HumanMessage(content=' Comment puis-je contacter le service client?', additional_kwargs={}, response_metadata={}), AIMessage(content='Vous pouvez contacter le service client par téléphone ou par e-mail.', additional_kwargs={}, response_metadata={})]\n",
      "Réponse : Vous pouvez contacter le service client par téléphone ou par e-mail.\n",
      "Erreur lors de la lecture audio : [Errno 2] No such file or directory: '/content/temp_audio.mp3'\n",
      "\n",
      "Options :\n",
      "1. Télécharger un fichier audio pour poser une question.\n",
      "2. Quitter.\n",
      "Au revoir !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjmYb6RNaex-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bfdSPCupI3Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVcA50dFpJQu"
   },
   "source": [
    "**gTTS (Google Text-to-Speech)** repose sur des modèles de **deep learning** pour générer la parole à partir du texte. Plus précisément, il utilise des **réseaux neuronaux profonds** pour effectuer la synthèse vocale. Ces modèles sont entraînés pour produire une voix naturelle et fluide.\n",
    "\n",
    "### Voici comment cela fonctionne dans les grandes lignes :\n",
    "\n",
    "1. **Prétraitement du texte** :\n",
    "   - Le texte que vous fournissez est d'abord analysé et transformé en une forme qui peut être interprétée par les réseaux neuronaux. Cela inclut des étapes comme la normalisation du texte (conversion en minuscules, gestion des ponctuations, etc.) et parfois l'analyse phonétique.\n",
    "\n",
    "2. **Modèle phonétique et prosodique** :\n",
    "   - Le modèle de deep learning va transformer le texte en **phonèmes**, qui sont les unités sonores fondamentales du langage (par exemple, \"cat\" se décompose en /k/, /æ/, /t/).\n",
    "   - Le modèle apprend également la **prosodie**, c'est-à-dire l'intonation, le rythme et la cadence du discours, ce qui est essentiel pour rendre la voix naturelle et agréable.\n",
    "\n",
    "3. **Synthèse vocale avec un modèle génératif** :\n",
    "   - gTTS utilise un réseau neuronal pour générer l'audio à partir des phonèmes et de la prosodie.\n",
    "   - Cette étape repose sur des architectures de modèles génératifs comme les **WaveNet** ou d'autres types de réseaux de neurones récurrents, qui sont capables de synthétiser des sons réalistes à partir des phonèmes fournis.\n",
    "     - **WaveNet**, par exemple, est un modèle développé par DeepMind, une filiale de Google, qui génère des formes d'onde audio très réalistes en prédisant chaque échantillon sonore à partir des précédents.\n",
    "     - D'autres modèles modernes de synthèse vocale, comme ceux basés sur des **transformers**, peuvent aussi être utilisés pour améliorer la qualité de la voix générée.\n",
    "\n",
    "4. **Génération du fichier audio** :\n",
    "   - Une fois le texte transformé en audio, celui-ci est ensuite sauvegardé dans un fichier audio, généralement au format **MP3**.\n",
    "   - Le fichier audio est renvoyé à l'utilisateur via l'API de gTTS, que l'on peut télécharger et lire.\n",
    "\n",
    "### Conclusion :\n",
    "gTTS utilise des **modèles de deep learning**, en particulier des réseaux neuronaux profonds et des modèles génératifs, pour produire une parole de qualité à partir du texte. Ces modèles sont entraînés sur de grandes quantités de données vocales pour apprendre à reproduire des voix humaines naturelles, avec la prosodie et les nuances appropriées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tg19wlL8pPQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
